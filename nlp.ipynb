{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-level dependencies\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter\n",
    "from os import path\n",
    "\n",
    "# notebook config\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:40,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='volume1.txt'\n",
    "\n",
    "def read_txt_data(fname):\n",
    "    with open(os.getcwd()+'/' + fname,'r',) as f:\n",
    "        # this way of reading the file gives a list of lines.\n",
    "        data_text = f.read()\n",
    "        f.close\n",
    "    return data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_txt_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    #remove line breaks and numerics\n",
    "    text = text.replace('[^a-zA-ZʻāēīūĀĒĪŪ]', '').replace('\\n', ' ')\n",
    "    # remove punctuation first\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # convert to all lower\n",
    "    text = str.lower(text)\n",
    "    # tokenize by word\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # get stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # stem\n",
    "    #stems = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_and_stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# create a TF-IDF vectorizer:\n",
    "tfidf_vectorizer = TfidfVectorizer(#min_df=0.005,\n",
    "                             #max_df=0.7,\n",
    "                             max_features=500,\n",
    "                             tokenizer=tokenize_and_stem,\n",
    "                             ngram_range=(1,3),\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf_vectorizer.fit_transform(tokens)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the tf-idf cosine difference\n",
    "# this helps us cluster documents that might be similar\n",
    "dist = 1 - cosine_similarity(vectors)\n",
    "\n",
    "# use ward clustering to find similar docs; \n",
    "# cluster analysis as an analysis of variance problem instead of using distance metrics or measures of association\n",
    "# agglomerative clustering algorithm: start out at the leaves and work its way to the trunk, so to speak. \n",
    "# It looks for groups of leaves that it forms into branches, the branches into limbs and eventually into the trunk. \n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "#titles = (df['Title'] + ' ' + df['Card #'].astype(str) + ' ' + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_terms(k):\n",
    "    for i in range(k):\n",
    "        print(\"Cluster %d words:\\n\" % i, end='')\n",
    "        indices = centroids[i,:50]\n",
    "        t = [terms[i] for i in indices]\n",
    "        print(t)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(#min_df=0.005,\n",
    "                             #max_df=0.7,\n",
    "                             max_features=500,\n",
    "                             tokenizer=tokenize_and_stem,\n",
    "                             ngram_range=(1,3),\n",
    "                             use_idf=True)\n",
    "\n",
    "vectors = vectorizer.fit_transform(lower_nostop)\n",
    "\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place tf-idf values in a pandas data frame\n",
    "vec_tfidf_scores = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
    "vec_tfidf_scores.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the appropriate cluster number\n",
    "plt.figure(figsize=(16, 8))\n",
    "wcss = []\n",
    "for i in range(1, 50):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(vectors)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 50), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letʻs look at the n clusters around the elbow\n",
    "ks = [10]\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(vectors)\n",
    "    centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    indices = centroids[0,:5]\n",
    "    print('#################\\nCluster terms at k=%d:' % k)\n",
    "    print_top_terms(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# create a TF-IDF vectorizer:\n",
    "tfidf_vectorizer = TfidfVectorizer(#min_df=0.005,\n",
    "                             #max_df=0.7,\n",
    "                             max_features=500,\n",
    "                             tokenizer=tokenize_and_stem,\n",
    "                             ngram_range=(1,3),\n",
    "                             use_idf=True)\n",
    "# calculate TF-IDF\n",
    "# this gives a weight to words based on their frequency in a document and the inverse frequency across all documents\n",
    "# idea: words that are frequent in a document, but also very frequent in other documents, might just be noisy\n",
    "# idea: words that are frequent in a document, and infrequent across other docuemnts, can help determine the article's topic\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lower_nostop) #fit the vectorizer to synopses\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# now calculate the tf-idf cosine difference\n",
    "# this helps us cluster documents that might be similar\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# use ward clustering to find similar docs; \n",
    "# cluster analysis as an analysis of variance problem instead of using distance metrics or measures of association\n",
    "# agglomerative clustering algorithm: start out at the leaves and work its way to the trunk, so to speak. \n",
    "# It looks for groups of leaves that it forms into branches, the branches into limbs and eventually into the trunk. \n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "#titles = (df['Title'] + ' ' + df['Card #'].astype(str) + ' ' + df['Drawing']).tolist()\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "# visualize the linkage matrix with a dendogram\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", leaf_font_size=9);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
